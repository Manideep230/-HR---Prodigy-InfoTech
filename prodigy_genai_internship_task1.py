# -*- coding: utf-8 -*-
"""Prodigy GenAI Internship.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10b7DIrjvLPO93jFfIa-b4PTTApu7wv96

Train a model to generate coherent and contextually relevant text based on a given prompt. Starting with GPT-2, a transformer model developed by OpenAI, you will learn how to fine-tune the model on a custom dataset to create text that mimics the style and structure of your training data.
"""

pip install transformers datasets torch

from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling
import torch

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

def load_dataset(filepath, tokenizer):
    return TextDataset(
        tokenizer=tokenizer,
        file_path=filepath,
        block_size=128  # Length of each input sequence
    )

def tokenize_data(file_path):
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    tokenizer.pad_token = tokenizer.eos_token  # Handle padding
    dataset = load_dataset(file_path, tokenizer)
    return dataset, tokenizer

dataset_path = 'task1prodigydataset.txt'
dataset, tokenizer = tokenize_data(dataset_path)

model = GPT2LMHeadModel.from_pretrained('gpt2')
model.resize_token_embeddings(len(tokenizer))
model.to(device)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False
)

training_args = TrainingArguments(
    output_dir='./gpt2-finetuned',
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=500,
    save_total_limit=2,
    prediction_loss_only=True,
    logging_dir='./logs'
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset
)

trainer.train()

def generate_text(prompt):
    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)
    output = model.generate(
        inputs,
        max_length=50,
        num_return_sequences=1,
        no_repeat_ngram_size=2,
        temperature=0.7,
        top_p=0.9,
    )
    return tokenizer.decode(output[0], skip_special_tokens=True)

prompt = "Once upon a time"
print(generate_text(prompt))

model.save_pretrained('./gpt2-finetuned')
tokenizer.save_pretrained('./gpt2-finetuned')

from transformers import GPT2LMHeadModel, GPT2Tokenizer

model = GPT2LMHeadModel.from_pretrained('./gpt2-finetuned').to(device)
tokenizer = GPT2Tokenizer.from_pretrained('./gpt2-finetuned')

print(generate_text("In the future, AI will"))

